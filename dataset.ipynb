{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7674454b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_type</th>\n",
       "      <th>isic_id</th>\n",
       "      <th>attribution</th>\n",
       "      <th>copyright_license</th>\n",
       "      <th>image_manipulation</th>\n",
       "      <th>age_approx</th>\n",
       "      <th>sex</th>\n",
       "      <th>skin_tone_class</th>\n",
       "      <th>site</th>\n",
       "      <th>...</th>\n",
       "      <th>MONET_vasculature_vessels</th>\n",
       "      <th>MONET_erythema</th>\n",
       "      <th>MONET_pigmented</th>\n",
       "      <th>MONET_gel_water_drop_fluid_dermoscopy_liquid</th>\n",
       "      <th>MONET_skin_markings_pen_ink_purple_pen</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>label</th>\n",
       "      <th>image_path</th>\n",
       "      <th>metadata_vector</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IL_0000652</td>\n",
       "      <td>dermoscopic</td>\n",
       "      <td>ISIC_4671410</td>\n",
       "      <td>MILK study team</td>\n",
       "      <td>CC-BY-NC</td>\n",
       "      <td>instrument only</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016397</td>\n",
       "      <td>0.032357</td>\n",
       "      <td>0.847014</td>\n",
       "      <td>0.138121</td>\n",
       "      <td>0.148776</td>\n",
       "      <td>BCC</td>\n",
       "      <td>1</td>\n",
       "      <td>dataset/MILK10k_Training_Input\\IL_0000652\\ISIC...</td>\n",
       "      <td>[70.0, 1, 1, 3]</td>\n",
       "      <td>BCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IL_0003176</td>\n",
       "      <td>dermoscopic</td>\n",
       "      <td>ISIC_5371928</td>\n",
       "      <td>MILK study team</td>\n",
       "      <td>CC-BY-NC</td>\n",
       "      <td>instrument only</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367882</td>\n",
       "      <td>0.645776</td>\n",
       "      <td>0.122108</td>\n",
       "      <td>0.719937</td>\n",
       "      <td>0.329812</td>\n",
       "      <td>BCC</td>\n",
       "      <td>1</td>\n",
       "      <td>dataset/MILK10k_Training_Input\\IL_0003176\\ISIC...</td>\n",
       "      <td>[45.0, 0, 5, 3]</td>\n",
       "      <td>BCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IL_0004688</td>\n",
       "      <td>dermoscopic</td>\n",
       "      <td>ISIC_3624913</td>\n",
       "      <td>MILK study team</td>\n",
       "      <td>CC-BY-NC</td>\n",
       "      <td>instrument only</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196453</td>\n",
       "      <td>0.519808</td>\n",
       "      <td>0.058424</td>\n",
       "      <td>0.319812</td>\n",
       "      <td>0.262883</td>\n",
       "      <td>BCC</td>\n",
       "      <td>1</td>\n",
       "      <td>dataset/MILK10k_Training_Input\\IL_0004688\\ISIC...</td>\n",
       "      <td>[50.0, 1, 3, 4]</td>\n",
       "      <td>BCC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IL_0005081</td>\n",
       "      <td>dermoscopic</td>\n",
       "      <td>ISIC_5186409</td>\n",
       "      <td>MILK study team</td>\n",
       "      <td>CC-BY-NC</td>\n",
       "      <td>instrument only</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444257</td>\n",
       "      <td>0.272836</td>\n",
       "      <td>0.259830</td>\n",
       "      <td>0.384243</td>\n",
       "      <td>0.220156</td>\n",
       "      <td>SCCKA</td>\n",
       "      <td>9</td>\n",
       "      <td>dataset/MILK10k_Training_Input\\IL_0005081\\ISIC...</td>\n",
       "      <td>[45.0, 1, 3, 3]</td>\n",
       "      <td>SCCKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IL_0006177</td>\n",
       "      <td>dermoscopic</td>\n",
       "      <td>ISIC_1048297</td>\n",
       "      <td>MILK study team</td>\n",
       "      <td>CC-BY-NC</td>\n",
       "      <td>instrument only</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.391223</td>\n",
       "      <td>0.260695</td>\n",
       "      <td>0.276002</td>\n",
       "      <td>0.283951</td>\n",
       "      <td>0.661556</td>\n",
       "      <td>BCC</td>\n",
       "      <td>1</td>\n",
       "      <td>dataset/MILK10k_Training_Input\\IL_0006177\\ISIC...</td>\n",
       "      <td>[75.0, 1, 3, 7]</td>\n",
       "      <td>BCC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    lesion_id   image_type       isic_id      attribution copyright_license  \\\n",
       "0  IL_0000652  dermoscopic  ISIC_4671410  MILK study team          CC-BY-NC   \n",
       "1  IL_0003176  dermoscopic  ISIC_5371928  MILK study team          CC-BY-NC   \n",
       "2  IL_0004688  dermoscopic  ISIC_3624913  MILK study team          CC-BY-NC   \n",
       "3  IL_0005081  dermoscopic  ISIC_5186409  MILK study team          CC-BY-NC   \n",
       "4  IL_0006177  dermoscopic  ISIC_1048297  MILK study team          CC-BY-NC   \n",
       "\n",
       "  image_manipulation  age_approx  sex  skin_tone_class  site  ...  \\\n",
       "0    instrument only        70.0    1                1     3  ...   \n",
       "1    instrument only        45.0    0                5     3  ...   \n",
       "2    instrument only        50.0    1                3     4  ...   \n",
       "3    instrument only        45.0    1                3     3  ...   \n",
       "4    instrument only        75.0    1                3     7  ...   \n",
       "\n",
       "   MONET_vasculature_vessels  MONET_erythema  MONET_pigmented  \\\n",
       "0                   0.016397        0.032357         0.847014   \n",
       "1                   0.367882        0.645776         0.122108   \n",
       "2                   0.196453        0.519808         0.058424   \n",
       "3                   0.444257        0.272836         0.259830   \n",
       "4                   0.391223        0.260695         0.276002   \n",
       "\n",
       "   MONET_gel_water_drop_fluid_dermoscopy_liquid  \\\n",
       "0                                      0.138121   \n",
       "1                                      0.719937   \n",
       "2                                      0.319812   \n",
       "3                                      0.384243   \n",
       "4                                      0.283951   \n",
       "\n",
       "   MONET_skin_markings_pen_ink_purple_pen  diagnosis  label  \\\n",
       "0                                0.148776        BCC      1   \n",
       "1                                0.329812        BCC      1   \n",
       "2                                0.262883        BCC      1   \n",
       "3                                0.220156      SCCKA      9   \n",
       "4                                0.661556        BCC      1   \n",
       "\n",
       "                                          image_path  metadata_vector target  \n",
       "0  dataset/MILK10k_Training_Input\\IL_0000652\\ISIC...  [70.0, 1, 1, 3]    BCC  \n",
       "1  dataset/MILK10k_Training_Input\\IL_0003176\\ISIC...  [45.0, 0, 5, 3]    BCC  \n",
       "2  dataset/MILK10k_Training_Input\\IL_0004688\\ISIC...  [50.0, 1, 3, 4]    BCC  \n",
       "3  dataset/MILK10k_Training_Input\\IL_0005081\\ISIC...  [45.0, 1, 3, 3]  SCCKA  \n",
       "4  dataset/MILK10k_Training_Input\\IL_0006177\\ISIC...  [75.0, 1, 3, 7]    BCC  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load dataset\n",
    "# -------------------------------\n",
    "df = pd.read_csv(\"dataset/milk10k_stage1_full.csv\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Define class columns\n",
    "# -------------------------------\n",
    "class_cols = [\n",
    "    \"AKIEC\", \"BCC\", \"BEN_OTH\", \"BKL\", \"DF\",\n",
    "    \"INF\", \"MAL_OTH\", \"MEL\", \"NV\", \"SCCKA\", \"VASC\"\n",
    "]\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Create a single target column\n",
    "# -------------------------------\n",
    "df[\"target\"] = df[class_cols].idxmax(axis=1)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Drop old one-hot encoded columns\n",
    "# -------------------------------\n",
    "df_clean = df.drop(columns=class_cols)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. OPTIONAL: Save cleaned dataset\n",
    "# -------------------------------\n",
    "df_clean.to_csv(\"milk10k_stage1_clean.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2217f520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AKIEC': np.float64(1.5721572157215722),\n",
       " 'BCC': np.float64(0.18888328166678683),\n",
       " 'BEN_OTH': np.float64(10.826446280991735),\n",
       " 'BKL': np.float64(0.8756684491978609),\n",
       " 'DF': np.float64(9.160839160839162),\n",
       " 'INF': np.float64(9.527272727272727),\n",
       " 'MAL_OTH': np.float64(52.92929292929293),\n",
       " 'MEL': np.float64(1.0585858585858585),\n",
       " 'NV': np.float64(0.6385571533024617),\n",
       " 'SCCKA': np.float64(1.0071112819527195),\n",
       " 'VASC': np.float64(10.135396518375241)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.unique(df[\"target\"]),\n",
    "    y=df[\"target\"]\n",
    ")\n",
    "\n",
    "dict(zip(np.unique(df[\"target\"]), weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1625215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "BCC        2018\n",
      "NV          597\n",
      "BKL         435\n",
      "SCCKA       378\n",
      "MEL         360\n",
      "INF         300\n",
      "DF          300\n",
      "VASC        300\n",
      "BEN_OTH     300\n",
      "MAL_OTH     300\n",
      "AKIEC       300\n",
      "Name: count, dtype: int64\n",
      "    lesion_id   image_type       isic_id      attribution copyright_license  \\\n",
      "0  IL_0972770  dermoscopic  ISIC_5214703  MILK study team          CC-BY-NC   \n",
      "1  IL_3673925  dermoscopic  ISIC_6898183  MILK study team          CC-BY-NC   \n",
      "2  IL_0375356  dermoscopic  ISIC_4117748  MILK study team          CC-BY-NC   \n",
      "3  IL_9439639  dermoscopic  ISIC_8126703  MILK study team          CC-BY-NC   \n",
      "4  IL_1366322  dermoscopic  ISIC_1209022  MILK study team          CC-BY-NC   \n",
      "\n",
      "  image_manipulation  age_approx  sex  skin_tone_class  site  ...  \\\n",
      "0    instrument only        25.0    1                4     6  ...   \n",
      "1    instrument only        55.0    0                2     7  ...   \n",
      "2    instrument only        75.0    1                2     3  ...   \n",
      "3    instrument only        60.0    0                2     3  ...   \n",
      "4    instrument only        80.0    0                3     3  ...   \n",
      "\n",
      "   MONET_vasculature_vessels  MONET_erythema  MONET_pigmented  \\\n",
      "0                   0.241988        0.833792         0.099484   \n",
      "1                   0.048653        0.116453         0.653312   \n",
      "2                   0.116584        0.447787         0.720346   \n",
      "3                   0.005066        0.011172         0.912353   \n",
      "4                   0.030743        0.305515         0.591605   \n",
      "\n",
      "   MONET_gel_water_drop_fluid_dermoscopy_liquid  \\\n",
      "0                                      0.489474   \n",
      "1                                      0.276670   \n",
      "2                                      0.249717   \n",
      "3                                      0.149256   \n",
      "4                                      0.281568   \n",
      "\n",
      "   MONET_skin_markings_pen_ink_purple_pen  diagnosis  label  \\\n",
      "0                                0.589459        INF      5   \n",
      "1                                0.528131         NV      8   \n",
      "2                                0.476139        MEL      7   \n",
      "3                                0.159633        MEL      7   \n",
      "4                                0.337023        BCC      1   \n",
      "\n",
      "                                          image_path  metadata_vector target  \n",
      "0  dataset/MILK10k_Training_Input\\IL_0972770\\ISIC...  [25.0, 1, 4, 6]    INF  \n",
      "1  dataset/MILK10k_Training_Input\\IL_3673925\\ISIC...  [55.0, 0, 2, 7]     NV  \n",
      "2  dataset/MILK10k_Training_Input\\IL_0375356\\ISIC...  [75.0, 1, 2, 3]    MEL  \n",
      "3  dataset/MILK10k_Training_Input\\IL_9439639\\ISIC...  [60.0, 0, 2, 3]    MEL  \n",
      "4  dataset/MILK10k_Training_Input\\IL_1366322\\ISIC...  [80.0, 0, 3, 3]    BCC  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Saved: train_balanced.csv and val.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv(\"milk10k_stage1_clean.csv\")\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Stratified Train/Val split\n",
    "# ------------------------------\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    stratify=df[\"target\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Oversample minority classes\n",
    "# ------------------------------\n",
    "TARGET_SAMPLES = 300      # desired min samples per class\n",
    "\n",
    "balanced_list = []\n",
    "\n",
    "for cls in train_df[\"target\"].unique():\n",
    "    cls_df = train_df[train_df[\"target\"] == cls]\n",
    "\n",
    "    if len(cls_df) < TARGET_SAMPLES:\n",
    "        # Oversample minority class\n",
    "        cls_df = resample(\n",
    "            cls_df,\n",
    "            replace=True,\n",
    "            n_samples=TARGET_SAMPLES,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    balanced_list.append(cls_df)\n",
    "\n",
    "# Final balanced training dataframe\n",
    "train_balanced = pd.concat(balanced_list).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ------------------------------\n",
    "# 3. OUTPUT\n",
    "# ------------------------------\n",
    "print(train_balanced[\"target\"].value_counts())\n",
    "print(train_balanced.head())\n",
    "\n",
    "# ------------------------------\n",
    "# 4. SAVE FILES\n",
    "# ------------------------------\n",
    "train_balanced.to_csv(\"train_balanced.csv\", index=False)\n",
    "val_df.to_csv(\"val.csv\", index=False)\n",
    "\n",
    "print(\"Saved: train_balanced.csv and val.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ea19c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class LesionMultimodalDataset(Dataset):\n",
    "    def __init__(self, df, transform=None, use_embeddings=True):\n",
    "        \"\"\"\n",
    "        df: DataFrame containing paths + features\n",
    "        transform: torchvision transforms for images\n",
    "        use_embeddings: whether to load embedding_0 ... embedding_4\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.use_embeddings = use_embeddings\n",
    "\n",
    "        # Identify MONET feature columns\n",
    "        self.monet_cols = [col for col in df.columns if col.startswith(\"MONET_\")]\n",
    "\n",
    "        # Embedding columns\n",
    "        if use_embeddings:\n",
    "            self.embed_cols = [f\"embedding_{i}\" for i in range(5)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # --------------------------\n",
    "        # 1. Load Image\n",
    "        # --------------------------\n",
    "        image_path = row[\"image_path\"]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(np.array(image)).permute(2,0,1) / 255.0  # fallback\n",
    "\n",
    "        # --------------------------\n",
    "        # 2. MONET clinical features\n",
    "        # --------------------------\n",
    "        monet_features = torch.tensor(row[self.monet_cols].values, dtype=torch.float32)\n",
    "\n",
    "        # --------------------------\n",
    "        # 3. metadata_vector (already a list like [age, sex, tone, site])\n",
    "        # --------------------------\n",
    "        metadata_vec = torch.tensor(eval(row[\"metadata_vector\"]), dtype=torch.float32)\n",
    "\n",
    "        # --------------------------\n",
    "        # 4. Embeddings (optional)\n",
    "        # --------------------------\n",
    "        if self.use_embeddings:\n",
    "            embed_vals = torch.tensor(row[self.embed_cols].values, dtype=torch.float32)\n",
    "        else:\n",
    "            embed_vals = torch.zeros(5)  # placeholder\n",
    "\n",
    "        # --------------------------\n",
    "        # 5. Target label (convert string â†’ integer)\n",
    "        # --------------------------\n",
    "        label_map = {\n",
    "            \"AKIEC\":0, \"BCC\":1, \"BEN_OTH\":2, \"BKL\":3, \"DF\":4, \n",
    "            \"INF\":5, \"MAL_OTH\":6, \"MEL\":7, \"NV\":8, \"SCCKA\":9, \"VASC\":10\n",
    "        }\n",
    "        label = torch.tensor(label_map[row[\"target\"]], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"monet\": monet_features,\n",
    "            \"meta\": metadata_vec,\n",
    "            \"embedding\": embed_vals,\n",
    "            \"label\": label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11c04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "MONET feature count: 7\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ==========================================================\n",
    "# âš¡ FORCE CUDA\n",
    "# ==========================================================\n",
    "assert torch.cuda.is_available(), \"CUDA GPU NOT FOUND!\"\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# ðŸ“Œ DATASET CLASS (Multimodal)\n",
    "# ==========================================================\n",
    "class LesionMultimodalDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.monet_cols = [c for c in df.columns if c.startswith(\"MONET_\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # ---------------------\n",
    "        # Image\n",
    "        # ---------------------\n",
    "        img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # ---------------------\n",
    "        # MONET features\n",
    "        # ---------------------\n",
    "        monet = torch.tensor(row[self.monet_cols].values, dtype=torch.float32)\n",
    "\n",
    "        # ---------------------\n",
    "        # Metadata\n",
    "        # ---------------------\n",
    "        meta = torch.tensor(eval(row[\"metadata_vector\"]), dtype=torch.float32)\n",
    "\n",
    "        # ---------------------\n",
    "        # Label\n",
    "        # ---------------------\n",
    "        label_map = {\n",
    "            \"AKIEC\":0,\"BCC\":1,\"BEN_OTH\":2,\"BKL\":3,\"DF\":4,\n",
    "            \"INF\":5,\"MAL_OTH\":6,\"MEL\":7,\"NV\":8,\"SCCKA\":9,\"VASC\":10\n",
    "        }\n",
    "        label = torch.tensor(label_map[row[\"target\"]], dtype=torch.long)\n",
    "\n",
    "        return {\"image\": img, \"monet\": monet, \"meta\": meta, \"label\": label}\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# ðŸ“Œ MULTIMODAL MODEL (EfficientNet-B3)\n",
    "# ==========================================================\n",
    "class MultimodalEfficientNetB3(nn.Module):\n",
    "    def __init__(self, monet_dim, meta_dim=4, num_classes=11):\n",
    "        super().__init__()\n",
    "\n",
    "        # ---------------------\n",
    "        # EfficientNet-B3 Backbone\n",
    "        # ---------------------\n",
    "        weights = EfficientNet_B3_Weights.IMAGENET1K_V1\n",
    "        effnet = efficientnet_b3(weights=weights)\n",
    "\n",
    "        in_feats = effnet.classifier[1].in_features\n",
    "        effnet.classifier = nn.Identity()\n",
    "        self.image_backbone = effnet\n",
    "\n",
    "        self.image_proj = nn.Sequential(\n",
    "            nn.Linear(in_feats, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "\n",
    "        # ---------------------\n",
    "        # MONET MLP\n",
    "        # ---------------------\n",
    "        self.monet_mlp = nn.Sequential(\n",
    "            nn.Linear(monet_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "        )\n",
    "\n",
    "        # ---------------------\n",
    "        # Metadata MLP\n",
    "        # ---------------------\n",
    "        self.meta_mlp = nn.Sequential(\n",
    "            nn.Linear(meta_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "        )\n",
    "\n",
    "        # ---------------------\n",
    "        # Fusion MLP\n",
    "        # ---------------------\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(512 + 128 + 32, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, monet, meta):\n",
    "        x_img = self.image_backbone(image)\n",
    "        x_img = self.image_proj(x_img)\n",
    "\n",
    "        x_monet = self.monet_mlp(monet)\n",
    "        x_meta = self.meta_mlp(meta)\n",
    "\n",
    "        x = torch.cat([x_img, x_monet, x_meta], dim=1)\n",
    "        return self.fusion(x)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# ðŸ“Œ LOAD DATA\n",
    "# ==========================================================\n",
    "train_df = pd.read_csv(\"train_balanced.csv\")\n",
    "val_df   = pd.read_csv(\"val.csv\")\n",
    "\n",
    "monet_cols = [c for c in train_df.columns if c.startswith(\"MONET_\")]\n",
    "monet_dim = len(monet_cols)\n",
    "print(\"MONET feature count:\", monet_dim)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# ðŸ“Œ TRANSFORMS\n",
    "# ==========================================================\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((300,300)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((300,300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# ðŸ“Œ DATALOADERS\n",
    "# ==========================================================\n",
    "train_ds = LesionMultimodalDataset(train_df, train_tf)\n",
    "val_ds   = LesionMultimodalDataset(val_df, val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# ðŸ“Œ MODEL + LOSS + OPTIMIZER\n",
    "# ==========================================================\n",
    "model = MultimodalEfficientNetB3(monet_dim).to(device)\n",
    "\n",
    "# class weights\n",
    "weights_dict = {\n",
    " 'AKIEC':1.5721572,'BCC':0.18888328,'BEN_OTH':10.82644628,'BKL':0.87566845,\n",
    " 'DF':9.16083916,'INF':9.52727272,'MAL_OTH':52.92929292,'MEL':1.05858585,\n",
    " 'NV':0.63855715,'SCCKA':1.00711128,'VASC':10.13539651\n",
    "}\n",
    "class_order = [\"AKIEC\",\"BCC\",\"BEN_OTH\",\"BKL\",\"DF\",\"INF\",\"MAL_OTH\",\"MEL\",\"NV\",\"SCCKA\",\"VASC\"]\n",
    "w = torch.tensor([weights_dict[c] for c in class_order], dtype=torch.float32).cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=w)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-4)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# ðŸ“Œ TRAINING + SAVE BEST MODEL\n",
    "# ==========================================================\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        img  = batch[\"image\"].cuda()\n",
    "        mon  = batch[\"monet\"].cuda()\n",
    "        meta = batch[\"meta\"].cuda()\n",
    "        lbl  = batch[\"label\"].cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(img, mon, meta)\n",
    "        loss = criterion(logits, lbl)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == lbl).sum().item()\n",
    "        total += lbl.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    print(f\"[Epoch {epoch+1}] Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    # ----------------- Validation -----------------\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            img  = batch[\"image\"].cuda()\n",
    "            mon  = batch[\"monet\"].cuda()\n",
    "            meta = batch[\"meta\"].cuda()\n",
    "            lbl  = batch[\"label\"].cuda()\n",
    "\n",
    "            logits = model(img, mon, meta)\n",
    "            pred = logits.argmax(1)\n",
    "            correct += (pred == lbl).sum().item()\n",
    "            total += lbl.size(0)\n",
    "\n",
    "    val_acc = correct / total\n",
    "    print(f\"[Epoch {epoch+1}] Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # SAVE BEST MODEL\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save({\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"epoch\": epoch,\n",
    "        }, \"best_multimodal_effb3.pth\")\n",
    "\n",
    "        print(\"ðŸ”¥ Saved best model -> best_multimodal_effb3.pth\")\n",
    "\n",
    "\n",
    "print(\"Training complete! Best Val Acc:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac81a05b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileUpload, VBox, Button\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# LOAD TRAINED MODEL\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# ========================================\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from ipywidgets import FileUpload, VBox, Button\n",
    "from IPython.display import display\n",
    "\n",
    "# ========================================\n",
    "# LOAD TRAINED MODEL\n",
    "# ========================================\n",
    "from train import MultimodalEfficientNetB3   # your model definition\n",
    "\n",
    "CHECKPOINT = \"best_multimodal_effb3.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MONET_DIM = 20   # <-- Update if different\n",
    "\n",
    "model = MultimodalEfficientNetB3(monet_dim=MONET_DIM).to(DEVICE)\n",
    "checkpoint = torch.load(CHECKPOINT, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "print(\"Model Loaded âœ”\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# GRAD-CAM IMPLEMENTATION\n",
    "# ========================================\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        target_layer.register_forward_hook(self.save_activation)\n",
    "        target_layer.register_backward_hook(self.save_gradient)\n",
    "\n",
    "    def save_activation(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def generate(self, img_tensor, monet, meta):\n",
    "        logits = self.model(img_tensor, monet, meta)\n",
    "        pred_class = logits.argmax(dim=1).item()\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        logits[0, pred_class].backward()\n",
    "\n",
    "        gradients = self.gradients[0]       # [C,H,W]\n",
    "        activations = self.activations[0]   # [C,H,W]\n",
    "\n",
    "        weights = gradients.mean(dim=(1,2))  # GAP\n",
    "\n",
    "        cam = torch.zeros(activations.shape[1:], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "        for w, act in zip(weights, activations):\n",
    "            cam += w * act\n",
    "\n",
    "        cam = torch.relu(cam)\n",
    "        cam = cam.cpu().detach().numpy()\n",
    "        cam = (cam - cam.min()) / (cam.max() + 1e-8)\n",
    "\n",
    "        return cam, pred_class\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# IMAGE TRANSFORM\n",
    "# ========================================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((300,300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# UPLOAD WIDGET\n",
    "# ========================================\n",
    "uploader = FileUpload(accept='.jpg,.png,.jpeg', multiple=False)\n",
    "\n",
    "def on_upload_change(change):\n",
    "    if uploader.value:\n",
    "        file = list(uploader.value.values())[0]\n",
    "        img_bytes = file['content']\n",
    "        img = Image.open(BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "        # Display uploaded image\n",
    "        plt.figure(figsize=(4,4))\n",
    "        plt.imshow(img)\n",
    "        plt.title(\"Uploaded Image\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        run_gradcam_on_image(img)\n",
    "\n",
    "\n",
    "def run_gradcam_on_image(img):\n",
    "    orig = np.array(img)\n",
    "    img_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Dummy MONET + metadata for GradCAM\n",
    "    monet = torch.zeros((1, MONET_DIM)).float().to(DEVICE)\n",
    "    meta  = torch.tensor([[30,1,3,5]], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    # GradCAM target\n",
    "    target_layer = model.image_backbone.features[-1]\n",
    "    cam_gen = GradCAM(model, target_layer)\n",
    "\n",
    "    cam, pred_class = cam_gen.generate(img_tensor, monet, meta)\n",
    "\n",
    "    cam_resized = cv2.resize(cam, (orig.shape[1], orig.shape[0]))\n",
    "    heatmap = cv2.applyColorMap(np.uint8(cam_resized * 255), cv2.COLORMAP_JET)\n",
    "    overlay = (orig * 0.6 + heatmap * 0.4).astype(np.uint8)\n",
    "\n",
    "    # Display results\n",
    "    plt.figure(figsize=(15,5))\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(orig)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(cam_resized, cmap=\"jet\")\n",
    "    plt.title(\"Grad-CAM Heatmap\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(overlay)\n",
    "    plt.title(\"Overlay\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Predicted Class Index: {pred_class}\")\n",
    "\n",
    "\n",
    "uploader.observe(on_upload_change, names='value')\n",
    "\n",
    "display(VBox([uploader]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
